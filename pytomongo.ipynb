{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymongo\n",
      "  Downloading pymongo-3.11.0-cp38-cp38-manylinux2014_x86_64.whl (530 kB)\n",
      "\u001b[K     |████████████████████████████████| 530 kB 786 kB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pymongo\n",
      "Successfully installed pymongo-3.11.0\n",
      "Collecting lxml\n",
      "  Downloading lxml-4.5.2-cp38-cp38-manylinux1_x86_64.whl (5.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.4 MB 3.4 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: lxml\n",
      "Successfully installed lxml-4.5.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pymongo\n",
    "!pip install lxml \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import json\n",
    "import os\n",
    "import re, time, requests\n",
    "from lxml import etree\n",
    "from multiprocessing import Pool\n",
    "from datetime import date\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_params(sceng= 0,area = '',min = '',max= ''):\n",
    "    paramslist = []\n",
    "    my_params = {'scneg': sceng,  # 搜尋參數\n",
    "                 'area': area,  # 指定區域\n",
    "                 'scmin': min,\n",
    "                 'scmax': max\n",
    "                 }\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.135 Safari/537.36',\n",
    "        'Referer': 'https://www.104.com.tw/job/'}\n",
    "    searchpageurl = 'https://www.104.com.tw/jobs/search/?'\n",
    "    ss = requests.session()\n",
    "\n",
    "    res = ss.get(url=searchpageurl, headers=headers, params=my_params)\n",
    "    html = etree.HTML(res.text)\n",
    "    try:\n",
    "        totalpage = int(html.xpath('//script[contains(text(),\"totalPage\")]/text()')[0].split('totalPage\":')[-1].split(\n",
    "            ',\"totalCount\"')[0])  # 取得js中的total page\n",
    "        print(\"totalPage:\", totalpage)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        totalpage = 1\n",
    "    for page in range(1,totalpage+1):\n",
    "        my_params = {'scneg': sceng,  # 搜尋參數\n",
    "                     'area': area,  # 指定區域\n",
    "                     'scmin': min,\n",
    "                     'scmax': max,\n",
    "                     'page': page\n",
    "                     }\n",
    "        paramslist.append(my_params)\n",
    "    return paramslist\n",
    "\n",
    "#帶入myparams參數取得後五碼的list\n",
    "def getindex(my_params):\n",
    "    joburl = []\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.4147.135 Safari/537.36',\n",
    "        'Referer': 'https://www.104.com.tw/job/'}\n",
    "    searchpageurl = 'https://www.104.com.tw/jobs/search/?'\n",
    "    ss = requests.session()\n",
    "\n",
    "    try:\n",
    "        res = ss.post(url=searchpageurl, headers=headers, params=my_params)#allow_redirects=False\n",
    "    except Exception as e:\n",
    "        print(e,\"continuing...\")\n",
    "    try:\n",
    "        if res.status_code == 200:\n",
    "            html = etree.HTML(res.text)\n",
    "            for ajax_content in html.xpath('//a[contains(@class,\"js-job-link\")]/@href'):\n",
    "                joburl.append(ajax_content.split('?')[0].split('/')[-1])\n",
    "            return joburl  #list\n",
    "\n",
    "        else :\n",
    "            print(\"wrong code :\",res.status_code)\n",
    "    except Exception as exc:\n",
    "        print(exc)\n",
    "#把抓下來的後五碼存進json檔\n",
    "def dump_json_file(query_dict):\n",
    "    dumped_json_cache = json.dumps(query_dict)\n",
    "    filename = date.today().strftime(\"%Y-%m-%d\")\n",
    "    fw = open('./{}.json'.format(filename), \"w\")\n",
    "    fw.write(dumped_json_cache)\n",
    "    fw.close()\n",
    "    print('dump the data successfully')\n",
    "\n",
    "def crowl(url):\n",
    "    tmpdict = {}\n",
    "    headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.135 Safari/537.36',\n",
    "    'Referer': 'https://www.104.com.tw/job/'}\n",
    "\n",
    "    ss=requests.session()\n",
    "    res = ss.get(url= url , headers = headers)\n",
    "    if res.status_code == 200:\n",
    "        try:\n",
    "            return res.json()\n",
    "        except json.decoder.JSONDecodeError:\n",
    "            print(res.json)\n",
    "\n",
    "    else:\n",
    "        print('請求網頁json錯誤, 錯誤狀態碼：', res.status_code)\n",
    "\n",
    "#資料清洗、重新排版\n",
    "def extract(joburl,data_dict):\n",
    "    job_dict = {joburl: {}}\n",
    "    #print('PID:{} , task:{}'.format(os.getpid(),joburl))\n",
    "    try:\n",
    "        data_dict = data_dict['data']\n",
    "\n",
    "        # job title\n",
    "        job_dict[joburl]['jobName'] = data_dict['header']['jobName']\n",
    "        job_dict[joburl]['appearDate'] = data_dict['header']['appearDate']\n",
    "\n",
    "        # company detail\n",
    "        job_dict[joburl]['companyName'] = data_dict['header']['custName']\n",
    "        job_dict[joburl]['companyUrl'] = data_dict['header']['custUrl']\n",
    "        job_dict[joburl]['industry'] = data_dict['industry']\n",
    "        job_dict[joburl]['addressRegion'] = data_dict['jobDetail']['addressRegion']\n",
    "        job_dict[joburl]['longitude'] = data_dict['jobDetail']['longitude']\n",
    "        job_dict[joburl]['latitude'] = data_dict['jobDetail']['latitude']\n",
    "\n",
    "        # condition\n",
    "        job_dict[joburl]['acceptRole'] = data_dict['condition']['acceptRole']\n",
    "        job_dict[joburl]['workExp'] = data_dict['condition']['workExp']\n",
    "        job_dict[joburl]['edu'] = data_dict['condition']['edu']\n",
    "        job_dict[joburl]['major'] = data_dict['condition']['major']\n",
    "        job_dict[joburl]['language'] = data_dict['condition']['language']\n",
    "        job_dict[joburl]['skill'] = data_dict['condition']['skill']\n",
    "        job_dict[joburl]['certificate'] = data_dict['condition']['certificate']\n",
    "        job_dict[joburl]['other'] = data_dict['condition']['other']\n",
    "\n",
    "        # job Detail\n",
    "        job_dict[joburl]['jobDescription'] = data_dict['jobDetail']['jobDescription']\n",
    "        job_dict[joburl]['jobCategory'] = data_dict['jobDetail']['jobCategory']\n",
    "        job_dict[joburl]['jobType'] = data_dict['jobDetail']['jobType']\n",
    "        job_dict[joburl]['manageResp'] = data_dict['jobDetail']['manageResp']\n",
    "        job_dict[joburl]['businessTrip'] = data_dict['jobDetail']['businessTrip']\n",
    "        job_dict[joburl]['workPeriod'] = data_dict['jobDetail']['workPeriod']\n",
    "        job_dict[joburl]['vacationPolicy'] = data_dict['jobDetail']['vacationPolicy']\n",
    "        job_dict[joburl]['startWorkingDay'] = data_dict['jobDetail']['startWorkingDay']\n",
    "        job_dict[joburl]['needEmp'] = data_dict['jobDetail']['needEmp']\n",
    "\n",
    "        # salary\n",
    "        job_dict[joburl]['salary'] = data_dict['jobDetail']['salary']\n",
    "        job_dict[joburl]['salaryMin'] = data_dict['jobDetail']['salaryMin']\n",
    "        job_dict[joburl]['salaryMax'] = data_dict['jobDetail']['salaryMax']\n",
    "        job_dict[joburl]['salaryType'] = data_dict['jobDetail']['salaryType']\n",
    "        job_dict[joburl]['welfare'] = data_dict['welfare']['welfare']\n",
    "\n",
    "        return job_dict\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(joburl)\n",
    "        return job_dict\n",
    "\n",
    "def write_json(filename,tmpdict):\n",
    "    #pprint.pprint(tmpdict)\n",
    "    with open('./'+filename,'w') as f:\n",
    "        f.write(json.dumps(tmpdict))\n",
    "    print(\"write{} successfully\".format(filename))\n",
    "\n",
    "def open_json_file(CACHE_FNAME):\n",
    "    try:\n",
    "        cache_file = open(CACHE_FNAME, 'r')\n",
    "        cache_contents = cache_file.read()\n",
    "        CACHE_DICTION = json.loads(cache_contents)\n",
    "        cache_file.close()\n",
    "        return CACHE_DICTION\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"no any cache:\",e)\n",
    "        CACHE_DICTION = {}\n",
    "        return CACHE_DICTION\n",
    "\n",
    "# 跟mongodb建立連線\n",
    "def mongo_connect_build():\n",
    "    global mycol\n",
    "    client = pymongo.MongoClient(host=\"mongodb\", port=27017)\n",
    "    # client = pymongo.MongoClient(host=\"mongodb\", port=27017)\n",
    "\n",
    "    # 選擇使用的db,不存在則會在資料輸入時自動建立\n",
    "    db = client['Topic_104']\n",
    "\n",
    "    # 選擇collection,不存在則會在資料輸入時自動建立\n",
    "    mycol = db[\"Jobs\"]\n",
    "\n",
    "\n",
    "def getlist():\n",
    "    if not os.path.exists('./data/'):\n",
    "        os.mkdir('./data/')\n",
    "    else:\n",
    "        pass\n",
    "    try:\n",
    "        l = catch()['index']\n",
    "    except TypeError:\n",
    "        print(\"doesn't need match...........\")\n",
    "        l=[]\n",
    "    print(\"please wait...\")\n",
    "    for i in os.listdir('./data/'):\n",
    "        if i.strip('.json') in l:\n",
    "            #del list[list.index(i.strip('.json'))]\n",
    "            l.remove(i.strip('.json'))\n",
    "    print(\"got catching list!!\",l)\n",
    "    return l\n",
    "\n",
    "\n",
    "def catch():\n",
    "\n",
    "    cachefilename = './{}.json'.format(date.today().strftime(\"%Y-%m-%d\"))  #檔案名稱為當天日期\n",
    "    try:\n",
    "        with open(cachefilename,'r') as f:\n",
    "            cache_contents = f.read()\n",
    "            cachedict = json.loads(cache_contents)\n",
    "        return cachedict\n",
    "    except FileNotFoundError :\n",
    "        print(\"---------no cache file-------------\")\n",
    "        return None\n",
    "\n",
    "def cach_to_json(index):\n",
    "    if len(index) < 10 :\n",
    "        url = \"https://www.104.com.tw/job/ajax/content/\"+index\n",
    "        try:\n",
    "            j = extract(index,crowl(url))\n",
    "            write_json('./data/{}.json'.format(index), j)\n",
    "            print(\"write:{}\".format(index))\n",
    "        except Exception as e:\n",
    "            print(\"error:\",e)\n",
    "            time.sleep(2)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "def main2():\n",
    "    arealist = [str(i) for i in range(6001001001, 6001001013)]  # 台北市區域\n",
    "    for i in range(6001002001, 6001002030):  # 新北市區域\n",
    "        arealist.append(str(i))\n",
    "    arealist=[6001002001,6001002002]\n",
    "    paramlist = []\n",
    "    print(\"paramslist making \")\n",
    "    for area in arealist:\n",
    "        paramlist += make_params(sceng=0, area=area, min=0, max=37999) # params(salary0~37999,所有地區)\n",
    "        #paramlist += make_params(sceng=1, area=area, min=38000, max=40000)\n",
    "        #paramlist += crowler104.make_params(sceng=0, area=area, min=40001, max='')\n",
    "    print(\"paramslist done\",paramlist)\n",
    "    print(\"paramslist length:\",len(paramlist))\n",
    "    print(\"getting index.....\")\n",
    "\n",
    "    index = []\n",
    "\n",
    "    for i in paramlist:\n",
    "        # try:\n",
    "        print(i)\n",
    "        print(getindex(i))\n",
    "        index += getindex(i)\n",
    "        # except :\n",
    "        #     #print(\"errors\",i)\n",
    "\n",
    "    indexjson = {'index': index} if index != [] or len(index)<10 else False\n",
    "    print('all done,total index amount:{}'.format(len(indexjson['index'])))\n",
    "    dump_json_file(indexjson)\n",
    "\n",
    "# 輸入資料\n",
    "def data_insert(CACHE_DICTION):\n",
    "    try:\n",
    "        mycol.insert_one(CACHE_DICTION)\n",
    "        print(\"insert successfully!：\",CACHE_DICTION.keys())\n",
    "    except Exception as e:\n",
    "        print(\"insert error:\",e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},

    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    main2()\n",
    "\n",
    "    list2 = getlist()\n",
    "    for i in list2:\n",
    "        cach_to_json(i)\n",
    "\n",
    "\n",
    "\n",
    "    mongo_connect_build() # 連線mongodb\n",
    "    # 班代設計程式爬下來之後,都會在data資料夾裡面,將各個檔案輸入到mongodb\n",
    "    for CACHE_FNAME in os.listdir('./data/'):\n",
    "        CACHE_DICTION = open_json_file('./data/'+CACHE_FNAME)\n",
    "        #print(CACHE_DICTION.keys())\n",
    "        \n",
    "        data_insert(CACHE_DICTION)\n",
    "\n",
    "\n",
    "    for i in mycol.find({},{\"_id\":0}).limit(10):\n",
    "        print(\"mongo top 10\",str(i.values()))\n",
    "    print(datetime.datetime.utcnow())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
